{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python_defaultSpec_1600724160729",
      "display_name": "Python 3.8.5 64-bit"
    },
    "colab": {
      "name": "model_building.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjidx090/Work/blob/main/notebooks/model_building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K8lPKQBRqgp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/lognorman20/stockx_competiton/refs/heads/master/data/Clean_Shoe_Data.csv"
      ],
      "metadata": {
        "id": "Dh70q4md57GZ",
        "outputId": "99583648-daee-4323-fa2f-b7c5888a5c95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-08 17:58:05--  https://raw.githubusercontent.com/lognorman20/stockx_competiton/refs/heads/master/data/Clean_Shoe_Data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8853857 (8.4M) [text/plain]\n",
            "Saving to: ‘Clean_Shoe_Data.csv.1’\n",
            "\n",
            "\rClean_Shoe_Data.csv   0%[                    ]       0  --.-KB/s               \rClean_Shoe_Data.csv 100%[===================>]   8.44M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-03-08 17:58:05 (125 MB/s) - ‘Clean_Shoe_Data.csv.1’ saved [8853857/8853857]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAohw-WrRqgt"
      },
      "source": [
        "# Reading in the data\n",
        "shoe_data = pd.read_csv('/content/Clean_Shoe_Data.csv', parse_dates = True)\n",
        "df = shoe_data.copy()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWO-hw8IRqgv"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biVpGX_PRqgw"
      },
      "source": [
        "# Renaming columns to get rid of spaces\n",
        "df = df.rename(columns={\n",
        "    \"Order Date\": \"Order_date\",\n",
        "    \"Sneaker Name\": \"Sneaker_Name\",\n",
        "    \"Sale Price\": \"Sale_Price\",\n",
        "    \"Retail Price\": \"Retail_Price\",\n",
        "    \"Release Date\": \"Release_Date\",\n",
        "    \"Shoe Size\": \"Shoe_Size\",\n",
        "    \"Buyer Region\": \"Buyer_Region\"\n",
        "    })"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o16VMq-Rqgy"
      },
      "source": [
        "# Converting dates into numericals\n",
        "import datetime as dt\n",
        "\n",
        "df['Order_date'] = pd.to_datetime(df['Order_date'])\n",
        "df['Order_date']=df['Order_date'].map(dt.datetime.toordinal)\n",
        "\n",
        "df['Release_Date'] = pd.to_datetime(df['Release_Date'])\n",
        "df['Release_Date']=df['Release_Date'].map(dt.datetime.toordinal)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smnP8bk1Rqg0"
      },
      "source": [
        "# Setting up train & test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(['Sale_Price'], axis=1)\n",
        "y = df.Sale_Price\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X, y, test_size=0.2, random_state = 27)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn.preprocessing"
      ],
      "metadata": {
        "id": "xwB_iNuB6RUX",
        "outputId": "32b9b824-4e2f-48fd-fc1c-85389496eb31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn.preprocessing\n",
            "  Downloading sklearn_preprocessing-0.1.0-py3-none-any.whl.metadata (70 bytes)\n",
            "Downloading sklearn_preprocessing-0.1.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: sklearn.preprocessing\n",
            "Successfully installed sklearn.preprocessing-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMD91-swRqg2",
        "outputId": "0f625339-c3e9-4fe4-8120-c32659a2ac03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Converting categorical data to numerical\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "object_cols = ['Sneaker_Name', 'Buyer_Region', 'Brand']\n",
        "# Apply one-hot encoder to each column with categorical data\n",
        "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
        "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
        "\n",
        "# One-hot encoding removed index; put it back\n",
        "OH_cols_train.index = X_train.index\n",
        "OH_cols_valid.index = X_valid.index\n",
        "\n",
        "# # Adding the column names after one hot encoding\n",
        "OH_cols_train.columns = OH_encoder.get_feature_names(object_cols)\n",
        "OH_cols_valid.columns = OH_encoder.get_feature_names(object_cols)\n",
        "\n",
        "# Remove categorical columns (will replace with one-hot encoding)\n",
        "num_X_train = X_train.drop(object_cols, axis=1)\n",
        "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
        "\n",
        "# Add one-hot encoded columns to numerical features\n",
        "X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
        "X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-243db459d863>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mobject_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Sneaker_Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Buyer_Region'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Brand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Apply one-hot encoder to each column with categorical data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mOH_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mOH_cols_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOH_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mOH_cols_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOH_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emx7RkFdRqg5"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR3dO_XBRqg9"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNGVbWxvRqhA"
      },
      "source": [
        "X_valid.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nWfIVH9RqhC"
      },
      "source": [
        "# Pipelines Creation\n",
        "## 1. RandomForestRegressor\n",
        "## 2. DecisionTreeRegressor\n",
        "## 3. XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqZ6HEGYRqhD"
      },
      "source": [
        "# Importing models\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "877MNCUERqhF"
      },
      "source": [
        "# Setting up pipelines\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Decision Tree Regression Pipeline\n",
        "pipeline_dtr=Pipeline([('dtr', DecisionTreeRegressor(random_state=27))])\n",
        "\n",
        "# Random Forest Pipeline\n",
        "pipeline_randomforest=Pipeline([('rf_regressor',RandomForestRegressor(random_state=27))])\n",
        "\n",
        "# XGBost Pipeline\n",
        "pipeline_xgb=Pipeline([('xgb_regressor',xgb.XGBRegressor(objective=\"reg:linear\", random_state=27))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "EVzOj8j1RqhH"
      },
      "source": [
        "# Creating a list of the pipelines to loop through them\n",
        "pipelines = [pipeline_dtr, pipeline_xgb, pipeline_randomforest]\n",
        "\n",
        "best_accuracy=0.0\n",
        "best_regressor=0\n",
        "best_pipeline=\"\"\n",
        "\n",
        "# Dictionary of pipelines and regression types for ease of reference\n",
        "pipe_dict = {0: 'DTR', 1: 'XGBoost', 2: 'RandomForest'}\n",
        "\n",
        "# Fit the pipelines\n",
        "for pipe in pipelines:\n",
        "\tpipe.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Checking the accuracy of each model\n",
        "for i,model in enumerate(pipelines):\n",
        "    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_valid,y_valid)))\n",
        "\n",
        "# Finding the best model\n",
        "for i,model in enumerate(pipelines):\n",
        "    if model.score(X_valid,y_valid)>best_accuracy:\n",
        "        best_accuracy=model.score(X_valid,y_valid)\n",
        "        best_pipeline=model\n",
        "        best_regressor=i\n",
        "print('Model with best accuracy: {}'.format(pipe_dict[best_regressor]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMEJuQPQRqhJ"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVIPM51yRqhJ"
      },
      "source": [
        "### Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikP-UUJHRqhK"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "X1h3DhnQRqhN"
      },
      "source": [
        "# Using Randomized Search CV to find the best parameters\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
        "# max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10, 15, 25, 50, 75, 100]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 5, 10]\n",
        "# Method of selecting samples for training each tree\n",
        "# bootstrap = [True, False]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf}\n",
        "\n",
        "pprint(random_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "iAcC3n1PRqhR"
      },
      "source": [
        "# THIS BLOCK TAKES EONS\n",
        "rf = RandomForestRegressor(random_state=27)\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=27, n_jobs = -1)\n",
        "\n",
        "# Fit the random search model\n",
        "\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDxFO2WqRqhT"
      },
      "source": [
        "# Evaluation of Random Search\n",
        "def evaluate(model, X_valid, y_valid):\n",
        "    predictions = model.predict(X_valid)\n",
        "    errors = np.sqrt(mean_squared_error(y_valid, predictions))\n",
        "    print('Model Performance')\n",
        "    print('MSE of: ', errors)\n",
        "\n",
        "    return errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "RsCcaa1oRqhV"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "base_model = rf\n",
        "base_model.fit(X_train, y_train)\n",
        "base_accuracy = evaluate(base_model, X_valid, y_valid)\n",
        "\n",
        "\n",
        "best_random = rf_random.best_estimator_\n",
        "best_random.fit(X_train , y_train)\n",
        "\n",
        "random_accuracy = evaluate(best_random, X_valid, y_valid)\n",
        "\n",
        "print('\\n')\n",
        "print('Base Model Error: ', base_accuracy)\n",
        "print('\\n')\n",
        "print('Improved Model Error: ', random_accuracy)\n",
        "print('Improvement of {:0.2f}%.'.format((random_accuracy - base_accuracy) / base_accuracy))\n",
        "\n",
        "print('\\n')\n",
        "print('RF_Randomized_Search_CV is complete.')\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Upo42MDbRqhX"
      },
      "source": [
        "print('The best model is',rf_random.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "9dG0Tq3jRqhZ"
      },
      "source": [
        "print('The accuracy of this model is', rf_random.best_score_*100 + '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "HYKXRaXZRqhc"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Multiply by -1 since sklearn calculates *negative* MAE\n",
        "scores = -1 * cross_val_score(best_random, X_valid, y_valid,\n",
        "                              cv=5,\n",
        "                              scoring='neg_mean_absolute_error')\n",
        "\n",
        "print(\"MAE scores:\\n\", scores)\n",
        "print(\"Average MAE score (across experiments):\\n\")\n",
        "print(scores.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh8L7i-ZRqhe"
      },
      "source": [
        "# Saving model to disk\n",
        "import pickle\n",
        "pickle.dump(best_random, open('model.pkl','wb'))\n",
        "\n",
        "# Loading model to compare the results\n",
        "model = pickle.load(open('model.pkl','rb'))\n",
        "\n",
        "# Loading model to compare the results\n",
        "model = pickle.load(open('model.pkl','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}